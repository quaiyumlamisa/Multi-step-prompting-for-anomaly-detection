{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQiajBahS18h",
        "outputId": "84494b3b-c4be-4a54-97e8-2dce7fa74751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” CHECKING LOGHUB REPOSITORY...\n",
            "================================================================================\n",
            "\n",
            "ðŸ“ Files available in HDFS folder:\n",
            "\n",
            "  âœ“ HDFS_2k.log\n",
            "    URL: https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log\n",
            "\n",
            "  âœ“ HDFS_2k.log_structured.csv\n",
            "    URL: https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log_structured.csv\n",
            "\n",
            "  âœ“ HDFS_2k.log_templates.csv\n",
            "    URL: https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log_templates.csv\n",
            "\n",
            "  âœ“ HDFS_templates.csv\n",
            "    URL: https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_templates.csv\n",
            "\n",
            "  âœ“ README.md\n",
            "    URL: https://raw.githubusercontent.com/logpai/loghub/master/HDFS/README.md\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "print(\"ðŸ” CHECKING LOGHUB REPOSITORY...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check what's in the HDFS folder\n",
        "api_url = \"https://api.github.com/repos/logpai/loghub/contents/HDFS\"\n",
        "response = requests.get(api_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    files = response.json()\n",
        "    print(\"\\nðŸ“ Files available in HDFS folder:\\n\")\n",
        "    for file in files:\n",
        "        print(f\"  âœ“ {file['name']}\")\n",
        "        print(f\"    URL: {file['download_url']}\\n\")\n",
        "else:\n",
        "    print(f\"âŒ Error: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log\"\n",
        "log_path = \"HDFS_2k.log\"\n",
        "\n",
        "print(\"â¬‡ï¸ Attempting download of HDFS_2k.log ...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Only download if file does not exist\n",
        "if not os.path.exists(log_path):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200 and len(response.content) > 0:\n",
        "        with open(log_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"âœ… Download complete:\", log_path)\n",
        "    else:\n",
        "        print(\"âŒ Download failed. Status code:\", response.status_code)\n",
        "        raise RuntimeError(\"Could not download dataset.\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ File already exists:\", log_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ke7nayzTMWz",
        "outputId": "f5bf09dc-2763-40cb-f479-ee2651e5f353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ Attempting download of HDFS_2k.log ...\n",
            "======================================================================\n",
            "âœ… Download complete: HDFS_2k.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"HDFS_2k.log\", \"r\") as f:\n",
        "    logs = f.readlines()\n",
        "\n",
        "print(\"Total lines:\", len(logs))\n",
        "print(\"\\nFirst 5 lines:\")\n",
        "for line in logs[:5]:\n",
        "    print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p3gWrs2TPxJ",
        "outputId": "05d5396f-4593-407f-c9cd-b72895478792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines: 2000\n",
            "\n",
            "First 5 lines:\n",
            "081109 203615 148 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_38865049064139660 terminating\n",
            "081109 203807 222 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6952295868487656571 terminating\n",
            "081109 204005 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864\n",
            "081109 204015 308 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8229193803249955061 terminating\n",
            "081109 204106 329 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6670958622368987959 terminating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets coverage google-genai\n",
        "\n",
        "import google.generativeai as genai\n",
        "# Configure Gemini API\n",
        "API_KEY = \"AIzaSyBrPJB2qhKmlg_U-5oV-RElZCw6BLpNB8M\"  # Replace this with your actual API key\n",
        "\n",
        "def setup_gemini(api_key):\n",
        "    genai.configure(api_key=api_key)\n",
        "    return genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "model = setup_gemini(API_KEY)\n",
        "print(\"Gemini API configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuN28YIYVPyD",
        "outputId": "2935346a-e28f-4405-c206-cd5e0e6018d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.12/dist-packages (7.13.0)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Gemini API configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def role_template_extractor(log_lines):\n",
        "    logs_block = \"\\n\".join(log_lines)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Extract log templates from these HDFS logs.\n",
        "\n",
        "RULES:\n",
        "- Keep log level + component name.\n",
        "- Replace IDs, numbers, IPs, ports, block IDs with <*>.\n",
        "- One template per unique structure.\n",
        "- Count occurrences.\n",
        "- Output ONLY valid JSON.\n",
        "\n",
        "LOGS:\n",
        "{logs_block}\n",
        "\n",
        "OUTPUT JSON EXAMPLE FORMAT:\n",
        "\"templates\": [],\n",
        "\"summary\": {{}}\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\"temperature\": 0}\n",
        "    )\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "qgJyMG7u6po2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def role_sequence_analyzer(log_lines, templates_json):\n",
        "    logs_block = \"\\n\".join(log_lines)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Analyze the event sequence of these HDFS logs.\n",
        "\n",
        "GOAL:\n",
        "- Map logs â†’ template IDs\n",
        "- Detect out-of-order or missing events\n",
        "- Detect unusual repetition\n",
        "- Compute anomaly_score (0 = normal, 1 = high)\n",
        "- Output JSON only\n",
        "\n",
        "TEMPLATES:\n",
        "{templates_json}\n",
        "\n",
        "LOGS:\n",
        "{logs_block}\n",
        "\n",
        "OUTPUT JSON KEYS:\n",
        "sequence, analysis, decision, anomaly_score\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\"temperature\": 0}\n",
        "    )\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "tGZnWFaFB_PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def role_anomaly_detector(log_lines, role1_output, role2_output):\n",
        "    logs_block = \"\\n\".join(log_lines)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Classify this HDFS log window as NORMAL or ANOMALOUS.\n",
        "\n",
        "USE THESE SIGNALS:\n",
        "- Strong: ERROR/WARN/CRITICAL, corruption, timeouts, invalid block operations\n",
        "- Moderate: rare templates, sequence anomaly, fast repetition\n",
        "- Weak: unusual template distribution, unusual parameter values\n",
        "\n",
        "DECISION RULES:\n",
        "- Anomalous if â‰¥1 strong OR â‰¥2 moderate OR â‰¥4 weak\n",
        "- Otherwise normal\n",
        "\n",
        "INPUT:\n",
        "Templates: {role1_output}\n",
        "Sequence: {role2_output}\n",
        "Logs:\n",
        "{logs_block}\n",
        "\n",
        "OUTPUT JSON ONLY:\n",
        "{{\n",
        "  \"classification\": \"\",\n",
        "  \"confidence\": 0.0,\n",
        "  \"triggered_indicators\": [],\n",
        "  \"explanation\": \"\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\"temperature\": 0}\n",
        "    )\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "RPCxP1XUCdEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def role_explanation_validator(log_lines, role1_output, role2_output, role3_output):\n",
        "    logs_block = \"\\n\".join(log_lines)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Validate the anomaly decision and provide the final explanation.\n",
        "\n",
        "CHECKS:\n",
        "- NORMAL â†’ sequence anomaly < 0.5\n",
        "- ANOMALOUS â†’ sequence anomaly > 0.5\n",
        "- Rare templates >= 3 â†’ suspicious\n",
        "- Error claims must match raw logs\n",
        "- Justify confidence\n",
        "\n",
        "INPUT:\n",
        "Role1: {role1_output}\n",
        "Role2: {role2_output}\n",
        "Role3: {role3_output}\n",
        "Logs:\n",
        "{logs_block}\n",
        "\n",
        "OUTPUT JSON KEYS:\n",
        "consistency_status, validation_score, final_decision, root_cause\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\"temperature\": 0}\n",
        "    )\n",
        "    return response.text\n",
        "\n"
      ],
      "metadata": {
        "id": "0N23VFDwCy_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 30\n",
        "\n",
        "def run_pipeline(logs, start_index):\n",
        "    # 1. Slice the window\n",
        "    window = logs[start_index : start_index + WINDOW_SIZE]\n",
        "\n",
        "    print(f\"\\nðŸš€ Running pipeline for window starting at index {start_index} ({len(window)} logs)\\n\")\n",
        "\n",
        "    # 2. Role 1 â€” Template Extraction\n",
        "    r1 = role_template_extractor(window)\n",
        "    print(\"Role 1 (Templates) Done\")\n",
        "\n",
        "    # 3. Role 2 â€” Sequence Analysis\n",
        "    r2 = role_sequence_analyzer(window, r1)\n",
        "    print(\"Role 2 (Sequence Analysis) Done\")\n",
        "\n",
        "    # 4. Role 3 â€” Anomaly Detector\n",
        "    r3 = role_anomaly_detector(window, r1, r2)\n",
        "    print(\"Role 3 (Anomaly Detection) Done\")\n",
        "\n",
        "    # 5. Role 4 â€” Final Validator\n",
        "    r4 = role_explanation_validator(window, r1, r2, r3)\n",
        "    print(\"Role 4 (Final Explanation) Done\")\n",
        "\n",
        "    # 6. Return all results\n",
        "    return {\n",
        "        \"templates\": r1,\n",
        "        \"sequence\": r2,\n",
        "        \"decision\": r3,\n",
        "        \"validator\": r4\n",
        "    }\n"
      ],
      "metadata": {
        "id": "eeEZV2t7DA37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def show_pipeline_output(out):\n",
        "    print(\"\\n================ ROLE 1: TEMPLATE EXTRACTION ================\\n\")\n",
        "    print(out[\"templates\"])\n",
        "\n",
        "    print(\"\\n================ ROLE 2: SEQUENCE ANALYSIS ================\\n\")\n",
        "    print(out[\"sequence\"])\n",
        "\n",
        "    print(\"\\n================ ROLE 3: ANOMALY DECISION ================\\n\")\n",
        "    print(out[\"decision\"])\n",
        "\n",
        "    print(\"\\n================ ROLE 4: FINAL VALIDATION ================\\n\")\n",
        "    print(out[\"validator\"])\n"
      ],
      "metadata": {
        "id": "7Zxb4rYWMgha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = run_pipeline(logs, start_index=0)\n",
        "show_pipeline_output(out)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Md6Y-diBDjhu",
        "outputId": "20e7b99c-0351-4dde-8d32-66bb50837970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Running pipeline for window starting at index 0 (30 logs)\n",
            "\n",
            "Role 1 (Templates) Done\n",
            "Role 2 (Sequence Analysis) Done\n",
            "Role 3 (Anomaly Detection) Done\n",
            "Role 4 (Final Explanation) Done\n",
            "\n",
            "================ ROLE 1: TEMPLATE EXTRACTION ================\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"templates\": [\n",
            "    \"INFO dfs.DataNode$PacketResponder: PacketResponder <*> for block <*> terminating\",\n",
            "    \"INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>::<*> is added to <*> size <*>\",\n",
            "    \"INFO dfs.DataNode$PacketResponder: Received block <*> of size <*> from /<*>\",\n",
            "    \"INFO dfs.DataNode$DataXceiver: Receiving block <*> src: /<*>::<*> dest: /<*>::<*>\",\n",
            "    \"INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: <*>. <*>\",\n",
            "    \"INFO dfs.DataBlockScanner: Verification succeeded for <*>\"\n",
            "  ],\n",
            "  \"summary\": {\n",
            "    \"INFO dfs.DataNode$PacketResponder: PacketResponder <*> for block <*> terminating\": 6,\n",
            "    \"INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>::<*> is added to <*> size <*>\": 9,\n",
            "    \"INFO dfs.DataNode$PacketResponder: Received block <*> of size <*> from /<*>\": 7,\n",
            "    \"INFO dfs.DataNode$DataXceiver: Receiving block <*> src: /<*>::<*> dest: /<*>::<*>\": 4,\n",
            "    \"INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: <*>. <*>\": 3,\n",
            "    \"INFO dfs.DataBlockScanner: Verification succeeded for <*>\": 1\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "================ ROLE 2: SEQUENCE ANALYSIS ================\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"sequence\": [\n",
            "    {\n",
            "      \"log_id\": 1,\n",
            "      \"template_id\": 0,\n",
            "      \"event\": \"PacketResponder terminating\",\n",
            "      \"block_id\": \"blk_38865049064139660\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 2,\n",
            "      \"template_id\": 0,\n",
            "      \"event\": \"PacketResponder terminating\",\n",
            "      \"block_id\": \"blk_-6952295868487656571\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 3,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_7128370237687728475\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 4,\n",
            "      \"template_id\": 0,\n",
            "      \"event\": \"PacketResponder terminating\",\n",
            "      \"block_id\": \"blk_8229193803249955061\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 5,\n",
            "      \"template_id\": 0,\n",
            "      \"event\": \"PacketResponder terminating\",\n",
            "      \"block_id\": \"blk_-6670958622368987959\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 6,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_3050920587428079149\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 7,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_7888946331804732825\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 8,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_2377150260128098806\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 9,\n",
            "      \"template_id\": 0,\n",
            "      \"event\": \"PacketResponder terminating\",\n",
            "      \"block_id\": \"blk_572492839287299681\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 10,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_3587508140051953248\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 11,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_5402003568334525940\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 12,\n",
            "      \"template_id\": 3,\n",
            "      \"event\": \"Receiving block\",\n",
            "      \"block_id\": \"blk_5792489080791696128\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 13,\n",
            "      \"template_id\": 3,\n",
            "      \"event\": \"Receiving block\",\n",
            "      \"block_id\": \"blk_1724757848743533110\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 14,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_8015913224713045110\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 15,\n",
            "      \"template_id\": 3,\n",
            "      \"event\": \"Receiving block\",\n",
            "      \"block_id\": \"blk_-5623176793330377570\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 16,\n",
            "      \"template_id\": 4,\n",
            "      \"event\": \"allocateBlock\",\n",
            "      \"block_id\": \"blk_-1727475099218615100\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 17,\n",
            "      \"template_id\": 0,\n",
            "      \"event\": \"PacketResponder terminating\",\n",
            "      \"block_id\": \"blk_5017373558217225674\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 18,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_9212264480425680329\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 19,\n",
            "      \"template_id\": 4,\n",
            "      \"event\": \"allocateBlock\",\n",
            "      \"block_id\": \"blk_-7878121102358435702\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 20,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_4568434182693165548\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 21,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_-5704899712662113150\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 22,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_-4794867979917102672\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 23,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_8763662564934652249\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 24,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_-5861636720645142679\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 25,\n",
            "      \"template_id\": 1,\n",
            "      \"event\": \"addStoredBlock\",\n",
            "      \"block_id\": \"blk_7453815855294711849\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 26,\n",
            "      \"template_id\": 3,\n",
            "      \"event\": \"Receiving block\",\n",
            "      \"block_id\": \"blk_-28342503914935090\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 27,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_8291449241650212794\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 28,\n",
            "      \"template_id\": 4,\n",
            "      \"event\": \"allocateBlock\",\n",
            "      \"block_id\": \"blk_-5319073033164653435\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 29,\n",
            "      \"template_id\": 5,\n",
            "      \"event\": \"Verification succeeded\",\n",
            "      \"block_id\": \"blk_-4980916519894289629\"\n",
            "    },\n",
            "    {\n",
            "      \"log_id\": 30,\n",
            "      \"template_id\": 2,\n",
            "      \"event\": \"Received block\",\n",
            "      \"block_id\": \"blk_-5974833545991408899\"\n",
            "    }\n",
            "  ],\n",
            "  \"analysis\": {\n",
            "    \"template_counts\": {\n",
            "      \"INFO dfs.DataNode$PacketResponder: PacketResponder <*> for block <*> terminating\": 6,\n",
            "      \"INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>::<*> is added to <*> size <*>\": 9,\n",
            "      \"INFO dfs.DataNode$PacketResponder: Received block <*> of size <*> from /<*>\": 7,\n",
            "      \"INFO dfs.DataNode$DataXceiver: Receiving block <*> src: /<*>::<*> dest: /<*>::<*>\": 4,\n",
            "      \"INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: <*>. <*>\": 3,\n",
            "      \"INFO dfs.DataBlockScanner: Verification succeeded for <*>\": 1\n",
            "    },\n",
            "    \"out_of_order_or_missing_events\": [\n",
            "      \"No strict out-of-order events detected for the *same block ID* within the observed log window (e.g., 'addStoredBlock' before 'Received block' for the same block).\",\n",
            "      \"However, a significant number of 'completion' or 'in-progress' events appear without their expected preceding events within this log window. This indicates a highly fragmented view of block operations, suggesting either the log window started mid-process for many operations, or there are missing logs from other nodes/earlier times:\",\n",
            "      \"- 6 'PacketResponder terminating' (Template 0) events lack a preceding 'addStoredBlock' (Template 1) for the same block ID in the observed window.\",\n",
            "      \"- 9 'addStoredBlock' (Template 1) events lack a preceding 'Received block' (Template 2) for the same block ID in the observed window.\",\n",
            "      \"- 7 'Received block' (Template 2) events lack a preceding 'Receiving block' (Template 3) for the same block ID in the observed window.\",\n",
            "      \"- 4 'Receiving block' (Template 3) events lack a preceding 'allocateBlock' (Template 4) for the same block ID in the observed window.\"\n",
            "    ],\n",
            "    \"unusual_repetition\": \"No unusual repetition patterns were observed. Sequences of similar events (e.g., multiple 'PacketResponder terminating' or 'addStoredBlock' events) are consistent with concurrent operations on different blocks in a distributed system.\"\n",
            "  },\n",
            "  \"decision\": \"The log stream exhibits significant fragmentation, with a high proportion of completion/in-progress events lacking their full preceding context within the observed window. While this can be a characteristic of partial log samples from a distributed system, it suggests an incomplete view of system operations. This fragmentation could potentially mask underlying issues or indicate inconsistencies in log collection, making comprehensive analysis challenging.\",\n",
            "  \"anomaly_score\": 0.6\n",
            "}\n",
            "```\n",
            "\n",
            "================ ROLE 3: ANOMALY DECISION ================\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"classification\": \"ANOMALOUS\",\n",
            "  \"confidence\": 0.9,\n",
            "  \"triggered_indicators\": [\n",
            "    \"Moderate: rare templates\",\n",
            "    \"Moderate: sequence anomaly\"\n",
            "  ],\n",
            "  \"explanation\": \"The log window is classified as ANOMALOUS because it exhibits two moderate signals:\\n1.  **Rare Template:** The template 'INFO dfs.DataBlockScanner: Verification succeeded for <*>' appears only once out of 30 log entries, which is a rare occurrence within this window.\\n2.  **Sequence Anomaly:** The analysis explicitly highlights 'out_of_order_or_missing_events'. A significant number of completion or in-progress events (e.g., 'PacketResponder terminating', 'addStoredBlock', 'Received block', 'Receiving block') lack their expected preceding events for the same block ID within the observed log window. This indicates a highly fragmented view of block operations and a deviation from expected event sequences.\\n\\nAccording to the decision rules, having â‰¥2 moderate signals (in this case, exactly 2) classifies the log window as ANOMALOUS.\"\n",
            "}\n",
            "```\n",
            "\n",
            "================ ROLE 4: FINAL VALIDATION ================\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"consistency_status\": \"CONSISTENT\",\n",
            "  \"validation_score\": 5,\n",
            "  \"final_decision\": \"ANOMALOUS\",\n",
            "  \"root_cause\": \"The log stream is anomalous due to significant fragmentation of block operation sequences and the rare occurrence of a 'Verification succeeded' event. A high number of 'completion' or 'in-progress' events (e.g., 'PacketResponder terminating', 'addStoredBlock', 'Received block', 'Receiving block') lack their expected preceding events for the same block ID within the observed log window. This indicates an incomplete or fragmented view of block operations, making it difficult to ascertain the full lifecycle of many blocks. Additionally, the 'INFO dfs.DataBlockScanner: Verification succeeded for <*>' template appears only once out of 30 log entries, which is a rare event within this specific log window. These two factors combined suggest an unusual pattern in the observed log activity.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}